{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mvt/.local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import tensorflow.compat.v1 as tf # we need convert tf version1 because some things are not available in version2 \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "We generate data using the exact same logic and code as the example from the previous notebook. The only difference now is that we save it to an npz file. Npz is numpy's file type which allows you to save numpy arrays into a single .npz file. We introduce this change because in machine learning most often:\n",
    "+ We are given some data (csv, database, etc.)\n",
    "+ We preprocess it into a desired format \n",
    "+ We save it into npz files to access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target is generate \"target = 2 * x - 3 * z + 5 + noise\"\n",
    "# Firstly, we need to generate observations \n",
    "obs = 1000\n",
    "\n",
    "xs = np.random.uniform(-10,10,(obs,1)) # (1000,1)\n",
    "zs = np.random.uniform(-10,10,(obs,1)) # (1000,1)\n",
    "\n",
    "# Combine the two dimensions of the input into one input matrix. \n",
    "# This is the X matrix from the linear model y = x*w + b.\n",
    "# column_stack is a Numpy method, which combines two matrices (vectors) into one.\n",
    "generated_inputs = np.column_stack((xs,zs)) # (1000,2)\n",
    "\n",
    "# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>noise \n",
    "noise = np.random.uniform(-1,1,(obs,1))\n",
    "\n",
    "# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.\n",
    "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
    "generated_targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# save into an npz file called \"TF_intro\"\n",
    "np.savez(os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"TF_intro\"), inputs=generated_inputs, targets=generated_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the data we've prepared above. Think about it as: number of inputs, number of outputs.\n",
    "input_size = 2 \n",
    "output_size = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a basic TensorFlow object - the placeholder.\n",
    "# As before, we will feed the inputs and targets to the model. \n",
    "# In the TensorFlow context, we feed the data to the model THROUGH the placeholders. \n",
    "# The particular inputs and targets are contained in our .npz file.\n",
    "\n",
    "# The first None parameter of the placeholders' shape means that\n",
    "# this dimension could be of any length. That's since we are mainly interested in\n",
    "# the input size, i.e. how many input variables we have and not the number of samples (observations)\n",
    "# The number of input variables changes the MODEL itself, while the number of observations doesn't.\n",
    "# Remember that the weights and biases were independent of the number of samples, so the MODEL is independent.\n",
    "# Important: NO calculation happens at this point.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# As before, we define our weights and biases.\n",
    "# They are the other basic TensorFlow object - a variable.\n",
    "# We feed data into placeholders and they have a different value for each iteration\n",
    "# Variables, however, preserve their values across iterations.\n",
    "# To sum up, data goes into placeholders; parameters go into variables.\n",
    "\n",
    "# We use the same random uniform initialization in [-0.1,0.1] as in the minimal example but using the TF syntax\n",
    "# Important: NO calculation happens at this point.\n",
    "weights = tf.Variable(tf.random_uniform([input_size,output_size], minval=-0.1,maxval=0.1))\n",
    "biases = tf.Variable(tf.random_uniform([output_size], minval=-0.1, maxval=0.1))\n",
    "\n",
    "# We get the outputs following our linear combination: y = xw + b\n",
    "# Important: NO calculation happens at this point.\n",
    "# This line simply tells TensorFlow what rule to apply when we feed in the training data (below).\n",
    "outputs = tf.matmul(inputs,weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the objective function and the optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we use a loss function, this time readily available, though.\n",
    "# mean_squared_error is the scaled L2-norm (per observation)\n",
    "# We divide by two to follow our earlier definitions. That doesn't really change anything.\n",
    "mean_loss = tf.losses.mean_squared_error(labels=targets, predictions=outputs) / 2 \n",
    "# Note that there also exists a function tf.nn.l2_loss. \n",
    "# tf.nn.l2_loss calculates the loss over all samples, instead of the average loss per sample.\n",
    "# Practically it's the same, a matter of preference.\n",
    "# The difference would be a smaller or larger learning rate to achieve the exact same result.\n",
    "\n",
    "# Instead of implementing Gradient Descent on our own, in TensorFlow we can simply state\n",
    "# \"Minimize the mean loss by using Gradient Descent with a given learning rate\"\n",
    "# Simple as that.\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we've defined the placeholders, variables, the loss function and the optimization method.\n",
    "# We have the structure for training, but we haven't trained anything yet.\n",
    "# The actual training (and subsequent implementation of the ML algorithm) happens inside sessions.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we start training, we need to initialize our variables: the weights and biases.\n",
    "# There is a specific method for initializing called global_variables_initializer().\n",
    "# Let's declare a variable \"initializer\" that will do that.\n",
    "initializer = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to initialize variables\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_data = np.load(os.path.join(os.path.expanduser(\"~\"), \"Desktop\" , \"TF_intro.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38387278\n",
      "0.3752343\n",
      "0.36693633\n",
      "0.35896587\n",
      "0.3513094\n",
      "0.34395495\n",
      "0.3368903\n",
      "0.33010462\n",
      "0.32358637\n",
      "0.31732515\n",
      "0.3113108\n",
      "0.30553365\n",
      "0.29998413\n",
      "0.2946535\n",
      "0.28953302\n",
      "0.28461444\n",
      "0.27989012\n",
      "0.2753518\n",
      "0.27099252\n",
      "0.2668051\n",
      "0.2627827\n",
      "0.25891897\n",
      "0.25520784\n",
      "0.2516427\n",
      "0.24821852\n",
      "0.24492902\n",
      "0.2417694\n",
      "0.23873432\n",
      "0.23581906\n",
      "0.23301862\n",
      "0.23032865\n",
      "0.22774465\n",
      "0.22526257\n",
      "0.22287852\n",
      "0.22058831\n",
      "0.21838872\n",
      "0.2162754\n",
      "0.21424572\n",
      "0.21229595\n",
      "0.21042307\n",
      "0.20862412\n",
      "0.20689596\n",
      "0.20523615\n",
      "0.2036418\n",
      "0.20211026\n",
      "0.2006391\n",
      "0.19922578\n",
      "0.19786844\n",
      "0.19656436\n",
      "0.19531201\n",
      "0.19410883\n",
      "0.19295321\n",
      "0.19184305\n",
      "0.19077666\n",
      "0.18975241\n",
      "0.18876848\n",
      "0.18782334\n",
      "0.18691552\n",
      "0.18604358\n",
      "0.18520597\n",
      "0.18440133\n",
      "0.18362844\n",
      "0.18288599\n",
      "0.18217291\n",
      "0.18148786\n",
      "0.18082996\n",
      "0.1801979\n",
      "0.17959079\n",
      "0.17900746\n",
      "0.17844735\n",
      "0.17790918\n",
      "0.17739232\n",
      "0.17689586\n",
      "0.17641895\n",
      "0.17596084\n",
      "0.17552081\n",
      "0.17509808\n",
      "0.17469206\n",
      "0.174302\n",
      "0.17392738\n",
      "0.17356747\n",
      "0.17322177\n",
      "0.17288974\n",
      "0.17257075\n",
      "0.17226437\n",
      "0.17197011\n",
      "0.1716874\n",
      "0.17141585\n",
      "0.171155\n",
      "0.17090438\n",
      "0.17066373\n",
      "0.1704325\n",
      "0.17021039\n",
      "0.16999708\n",
      "0.1697922\n",
      "0.16959539\n",
      "0.1694064\n",
      "0.1692248\n",
      "0.1690503\n",
      "0.16888271\n"
     ]
    }
   ],
   "source": [
    "for e in range(100) : \n",
    "    # This expression is a bit more complex but you'll learn to appreciate its power and\n",
    "    # flexibility in the following lessons.\n",
    "    # sess.run is the session's function to actually do something, anything.\n",
    "    # Above, we used it to initialize the variables.\n",
    "    # Here, we use it to feed the training data to the computational graph, defined by the feed_dict parameter\n",
    "    # and run operations (already defined above), given as the first parameter (optimize, mean_loss).\n",
    "    \n",
    "    # So the line of code means: \"Run the optimize and mean_loss operations by filling the placeholder\n",
    "    # objects with data from the feed_dict parameter\".\n",
    "    # Curr_loss catches the output from the two operations.\n",
    "    # Using \"_,\" we omit the first one, because optimize has no output (it's always \"None\"). \n",
    "    # The second one catches the value of the mean_loss for the current run, thus curr_loss actually = mean_loss\n",
    "    _ , curr_loss = sess.run([optimize,mean_loss], feed_dict={inputs : trainning_data[\"inputs\"], targets : trainning_data[\"targets\"]})\n",
    "    print(curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efc20ffa550>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As before, we want to plot the last output vs targets after the training is supposedly over.\n",
    "# Same notation as above but this time we don't want to train anymore, and we are not interested\n",
    "# in the loss function value.\n",
    "# What we want, however, are the outputs. \n",
    "# Therefore, instead of the optimize and mean_loss operations, we pass the \"outputs\" as the only parameter.\n",
    "predicts = sess.run([outputs], feed_dict={inputs : trainning_data[\"inputs\"]})\n",
    "# The model is optimized, so the outputs are calculated based on the last form of the model\n",
    "\n",
    "# We have to np.squeeze the arrays in order to fit them to what the plot function expects.\n",
    "# Doesn't change anything as we cut dimensions of size 1 - just a technicality.\n",
    "plt.scatter(np.squeeze(predicts), np.squeeze(trainning_data[\"targets\"]))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
